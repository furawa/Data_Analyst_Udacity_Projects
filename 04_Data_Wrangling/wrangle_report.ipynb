{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report  We Rate dogs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'images/weratedogs.jpg' alt = 'We rate dogs home photo' width = 50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction  \n",
    "\n",
    "In this project we gathered, analyzed and visualized the data of the Twitter @rate_dogs known as WeRateDogs. \n",
    "We will document the steps for the wrangling part. The wrangling process has 3 parts : Gather data, Assess Data and\n",
    "Clean Data. \n",
    "\n",
    "## Gather Data \n",
    "\n",
    "The data is being gathered from 3 differents sources :   \n",
    "    **The WeRateDogs Twitter archive** contains basic tweet data for all 5000+ of their tweets, but not everything. This twitter archive is being given to us and downloaded manually from the Udacity website.   \n",
    "    **The Image Predictions** File a table full of image predictions of breeds of dogs (the top three only) alongside each tweet ID, image URL and the image number that correspond to the most confident prediction.  \n",
    "This file has been downloaded programmatically using The Requests library.    \n",
    "    **The Twitter API** has been used to gathered additional data. We queried the Twitter API using the Tweepy Access library, for that we registered an account on the Twitter Developer Website, the account allow us to have access tokens to use with tweepy. For security reasons these access tokens are not shown in the jupyter notebook. After querying all the tweets which ID were present in the archive we saved it locally and loaded it with keeping just the *Tweet_id*, *favorite_count* and *retweet_count* columns.  \n",
    "\n",
    "## Assess Data\n",
    "\n",
    "We assessed the data visually and programmatically.    \n",
    "**visually**   \n",
    "We check for quality and tidiness issues visually detectable as invalid names of dogs, unconsistent columns and so on.    \n",
    "**programmatically**    \n",
    "Using attributes like pd.info(), pd.describe(), we can check another quality and tidiness issues as wrong type of the date column and so on.  \n",
    "Here are the **quality issues** found :    \n",
    "\n",
    "- The data type of the tweet_id is not the same in all three files\n",
    "- The data type of favorite_count and retweet_count in df_tweets is object \n",
    "- The number of rows in image_predictions and twitter_archive are differents\n",
    "- p1,p2,p3 variables in image_predictions do not start all with Capitalize letter and many values have underscore\n",
    "- The rating_numerator and rating_denominator have wrong values and should be float data type  \n",
    "- The timestamp and retweeted_status_timestamp are string data type but actually must be datetime \n",
    "- Not valid names for many dogs (\"a\", \"the\", \"an\")\n",
    "- We only want original ratings that have images.\n",
    "- Remove useless columns from the final dataframe\n",
    "- Rename columns of the final file for better understanding \n",
    "\n",
    "and the **Tidiness Issues**\n",
    "\n",
    "- The doggo, flooter, pupper and puppo columns must be in one column.\n",
    "- All 3 dataframes must be merge toghether in one dataframe  \n",
    "- Remove duplicates values from the final dataframe\n",
    "\n",
    "There are others quality and tidiness issues but we had not enough time to consider them all, these are the most evidents.  \n",
    "\n",
    "## Cleaning Data  \n",
    "The cleaning data process follow the same 3 phases for each issue :    \n",
    "\n",
    "- *Define*    \n",
    "Here we explain with words the process to resolve the issue  \n",
    "- *Code*      \n",
    "Here we programmatically resolve the issue  using python code  \n",
    "- *Test*      \n",
    "We make a test or assertion to verify if the issue is risolved  \n",
    "\n",
    "## Store the Data  \n",
    "After cleaning all the data and merge all the 3 files together, we store it in a master file that we will use for the analysis. The analysis and the insights found are in the `act_report.pdf`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "investigate",
   "language": "python",
   "name": "investigate_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
